# -*- coding: utf-8 -*-
# @Time    : 11/16/22 3:05 PM
# @Author  : LIANYONGXING
# @FileName: bert.py
# @Software: PyCharm
# @Repo    : https://github.com/lianyongxing/

import torch
import torch.nn as nn
from transformers import BertModel, BertConfig, BertTokenizer
import warnings

warnings.filterwarnings('ignore')

# bert_path = "/data/yxlian/tianran_data/chinese_bert"  # è¯¥æ–‡ä»¶å¤¹ä¸‹å­˜æ”¾ä¸‰ä¸ªæ–‡ä»¶ï¼ˆ'vocab.txt', 'pytorch_model.bin', 'config.json'ï¼‰

class Bert(nn.Module):

    def __init__(self, bert_path, classes=2):
        super(Bert, self).__init__()
        self.config = BertConfig.from_pretrained(bert_path)  # å¯¼å…¥æ¨¡å‹è¶…å‚æ•°
        self.bert = BertModel.from_pretrained(bert_path, output_attentions=True)  # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡
        self.fc = nn.Linear(self.config.hidden_size, classes)  # ç›´æ¥åˆ†ç±»
        self.tokenizer = BertTokenizer.from_pretrained(bert_path, tokenize_chinese_chars=True)
        self.DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.max_len = 128

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        outputs = self.bert(input_ids, attention_mask, token_type_ids)
        out_pool = outputs[1]  # æ± åŒ–åçš„è¾“å‡º [bs, config.hidden_size]
        logit = self.fc(out_pool)  # [bs, classes]
        return logit

    # def predict(self, raw_text):
    #     text = self.text_filtering(raw_text)
    #     if text == "":
    #         return 0, 0
    #     input_ids, input_masks, input_types = [], [], []  # input char ids, segment type ids, attention mask  # æ ‡ç­¾
    #     encode_dict = self.tokenizer.encode_plus(text, max_length=self.max_len, padding='max_length', truncation=True)
    #     input_ids.append(encode_dict['input_ids'])
    #     input_types.append(encode_dict['token_type_ids'])
    #     input_masks.append(encode_dict['attention_mask'])
    #     logits = self.forward(torch.LongTensor(input_ids).to(self.DEVICE),
    #                           torch.LongTensor(input_masks).to(self.DEVICE),
    #                           torch.LongTensor(input_types).to(self.DEVICE))
    #     y_pred_res = torch.argmax(logits, dim=1).detach().cpu().numpy().tolist()[0]
    #     y_pred_prob = F.softmax(logits, dim=1).detach().cpu().numpy()[0][1]
    #     return y_pred_res, y_pred_prob
    #
    # def predict_dataloader(self, data_loader):
    #     val_pred = []
    #     scores = []
    #     with torch.no_grad():
    #         for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):
    #             y_pred = self.forward(ids.to(self.DEVICE), att.to(self.DEVICE), tpe.to(self.DEVICE))
    #             y_pred_lab = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()
    #             y_pred_prob = F.softmax(y_pred, dim=1).detach().cpu().numpy()
    #             score = [y_pred_prob[idx, 1] for idx, i in enumerate(y_pred_lab)]
    #             #                 print(y_pred_prob)
    #             scores.extend(score)
    #             val_pred.extend(y_pred_lab)
    #     return val_pred, scores
    #
    # @staticmethod
    # def text_filtering(line):
    #     """
    #     filtering raw text
    #     Args:
    #         line:
    #             raw text
    #     Returns:
    #             filtered text
    #     """
    #     line = line.strip()
    #     line = re.sub(r'@[\w.?!,]+', '', line)
    #     for k in emojis.keys():
    #         line = line.replace(k, emojis[k])
    #     line = emojiswitch.demojize(line, delimiters=("", ""))
    #     line = re.sub(r'\[\w+\]', '', line)  # å»é™¤è¡¨æƒ…ï¼Œä¾‹å¦‚ï¼š[å¾®ç¬‘R]æˆ–è€…[æ³ª]
    #     line = line.replace("\xa0", '')  # å»é™¤\xa0
    #     line = re.sub(r'[^a-zA-Z0-9\u4e00-\u9fa5]', '', line).lower()  # å»é™¤é™¤äº†æ±‰å­—å¤§å°å†™å’Œæ•°å­—
    #     line = zhconv.convert(line, 'zh-cn')  # ç¹ä½“è½¬ç®€ä½“
    #     line = line[:(line + line).find(line, 1)]  # å»é™¤å¾ªç¯å­—ç¬¦ä¸²
    #     return line


# emojis = {"ğŸ’©": "å±",
#           "ğŸ§ ": "è„‘å­",
#           'ğŸ‘´': "çˆ·",
#           'ãŠ—ï¸': "ç¥",
#           'ğŸ·': "çŒª",
#           'ğŸƒ': "å¶",
#           'â¬†ï¸': "ä¸Š",
#           'ğŸ§ ': "è„‘å­",
#           'ğŸ¸': 'è›™',
#           "ğŸ»": 'ç†Š',
#           "ğŸˆ²ï¸": "ç¦",
#           "â©": "åŠ é€Ÿ",
#           "ğŸ¯": "èœ‚èœœ",
#           "1âƒ£ï¸": "ä¸€",
#           "7âƒ£ï¸": "ä¸ƒ",
#           "ğŸ´": "é©¬",
#           "â¤ï¸": "å¿ƒ",
#           "ğŸ‘": "èµ",
#           "ğŸ‘®â€â™€ï¸": "è­¦å¯Ÿ",
#           "ğŸ‘®": "è­¦å¯Ÿ",
#           "ğŸ­": "é¼ ",
#           "ğŸ¶": "ç‹—",
#           "ğŸ‘€": "çœ¼",
#           "ğŸ¢": "é¾Ÿ",
#           "ğŸŒ·": "",
#           "ğŸ’°": "é’±",
#           "ğŸ™Š": "",
#           "ğŸ ": "å°çº¢ä¹¦",
#           "ğŸ˜°": "",
#           "ğŸ™ğŸ»": "",
#           "ğŸ’ª": "",
#           "ğŸ‘": "",
#           "ğŸ‘äºº": "é˜³æ€§æ‚£è€…",
#           "ğŸš”": "è­¦",
#           "â˜‚ï¸": "ä¼",
#           "ğŸ€„ï¸": "ä¸­",
#           "ğŸŒ‚": "ä¼",
#           "ğŸ“–": "ä¹¦",
#           "ğŸ‘ŠğŸ»": "æ‹³æ‰“",
#           "ğŸ¥º": ""
#           }